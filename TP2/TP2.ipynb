{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"logo.png\", width=100, ALIGN=\"left\">\n",
    "<center>\n",
    "<h1>Mini Projets 2019-2020 (Info 232)</h1>\n",
    "Isabelle Guyon <br>\n",
    "info232@chalearn.org <br>\n",
    "</center>\n",
    "<span style=\"color:red\"> <h1> 2 . Pandas </h1> </span>\n",
    "    \n",
    "We have now a dataset of pictures of REAL apples and bananas, preprocessed in 4 different representations:\n",
    "- one with only 2 features (<b>R</b>edness and <b>E</b>longation) called <b>RE_data.csv</b>\n",
    "- one with only 21 features (<b>C</b>olor and <b>S</b>hape features) called <b>CS_data.csv</b>\n",
    "- one with 14580 features (all pixels of 81x69x3 <b>raw</b> images) called <b>RAW_data.csv</b>\n",
    "- one with 3072 features (all pixels of 32x32x3 <b>crop</b>ped images) called <b>CROP_data.csv</b>.\n",
    "\n",
    "We will compare the performances of various classifiers on those 4 datasets. Thus, we are going to start doing \"real\" POM: Probability, Optimization, and Modeling. We will proceed in a \"greedy\" way, eliminating some non promising avenues as we go, and not revisiting them for the moment:\n",
    "\n",
    "1. Which dataset version should we keep? Should we or not scale variables (using variable standardization)?\n",
    "2. Which learning machine of a standard toolkit (scikit-learn) is most promising?\n",
    "\n",
    "</div>\n",
    "<div style=\"background:#FFFFAA\">\n",
    "    \n",
    " This TP gives you 5 points if you answer well ALL 5 questions. If you cannot fisnish, get help by attending the Wednesday session.\n",
    "    \n",
    "<span style=\"color:red\"> <b>Save your notebook often with menu File + Save and Checkpoint.</b>\n",
    "<br> <b>Before you push your homework to your GitHub repo, use  Kernel + Restart and Run all.</b>\n",
    "</span>\n",
    "    </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general libraries\n",
    "import os, re\n",
    "from glob import glob as ls\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### Add path to the sample code so the notebook finds it:\n",
    "code_dir = 'code/'                        \n",
    "from sys import path; path.append(code_dir)\n",
    "from utilities import *\n",
    "# Import code that checks your answers\n",
    "from checker import check \n",
    "# Disable some warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Select data representation\n",
    "Which dataset version should we keep? Should we do or not a variable standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0: Examine the data directory\n",
    "Go to the directory `mini-dataset/`. You should find four data files ending with `.csv`. In an editor, open `RL_data.csv` or another of the data files to see how it looks like. The dataset is formatted in the CSV format (comma separated file). The examples are in lines and the features are separated by commas. The first line is the header. \n",
    "\n",
    "Examine all 4 datasets and note the number of lines and columns. Notice that they all have a different number of features but the same number of examples. \n",
    "\n",
    "It is somewhat easier to write a Python program to do this work for you. Check how the function `check_datasets` is written by typing `??check_datasets` in a new cell. Verify with an editor that this is the same code that is found in the directory `code/` in the file `utilities.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the datasets\n",
    "data_dir = './mini-dataset/'\n",
    "data_list = ls(data_dir + '*_data.csv')\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset sizes\n",
    "check_datasets(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Show the code of check_datasets.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Load and visualize data with Pandas (review)\n",
    "In previous classes we have have started using `Numpy arrays`and `Pandas dataframes`. We now explore Pandas a little bit further. Pandas dataframes are also arrays, but a different kind of python object that Numpy arrays. They have more properties, supporting fancy database functions and having quite a few display functions and nice simple summary statistics, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\">check the documentation</a>. Learning about Pandas will help a lot the <i>visualization binome</i>.\n",
    "\n",
    "First, you will perform these steps:\n",
    "* Call to UNIX command `!head ./mini-dataset/RE_data.csv` to view the first few lines of the file. The character `!` allows you to \"escape\" from the Jupyter notebook to the UNIX shell.\n",
    "* Load `RL_data.csv` as a pandas dataframe called `df`. Show the first few lines using the method `head`.\n",
    "* Compute simple statistics using the method `describe`.\n",
    "* Show the heat map. If you do not remember from the previous TP, think of using a search engine and type the keywords: \"pandas heatmap\". I found <a href=\"https://stackoverflow.com/questions/12286607/making-heatmap-from-pandas-dataframe\">this post</a>, for instance. \n",
    "* Create a new data frame called `df_scaled` obtained by standardizing the columns of `df`. In the previous TP we used the method `StandardScaler` of `sklearn.preprocessing`. Notice that you can also simply compute the mean of `df` with `df.mean()` and the standard deviation with `df.std()`, then in one line of code get `df_scaled` by performing algebraic operations on dataframes!\n",
    "* We actually doe NOT want to standardize the <b>last column</b> (the class label). Make sure the last column of  `df_scaled` has the ORIGINAL label values +1 or -1.\n",
    "\n",
    "Then, the question you should answer to complete this section is: what are the mean and standard deviation of the lines and the columns of the `redness` and `elongation` features before and after standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Put here your call to the UNIX command \"head\".</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Read file RE_data.csv as a dataframe and call it \"df\".</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4], 'col3': [5, 6]}) # REPLACE THIS\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Show descriptive statistics with the method \"describe\".</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Show the heat map of \"df\".</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Standardize the columns of \"df\", EXCEPT THE LAST ONE.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "df_scaled = df # REPLACE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Your final answers to question 1. </b> What are the mean and standard deviation of the lines and the columns of the `redness` and `elongation` features before and after standardization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 1\n",
    "score = 0\n",
    "# Put your answers to question 1 here\n",
    "redness_mean_before, elongation_mean_before, label_mean_before = 10, 10, 10\n",
    "redness_std_before, elongation_std_before, label_std_before = 10, 10, 10\n",
    "redness_mean_after, elongation_mean_after, label_mean_after = 10, 10, 10\n",
    "redness_std_after, elongation_std_after, label_std_after = 10, 10, 10\n",
    "\n",
    "# This is the checker code, keep it\n",
    "answer = redness_mean_before+elongation_mean_before+label_mean_before\n",
    "answer += redness_std_before+elongation_std_before+label_std_before\n",
    "answer -= redness_mean_after+elongation_mean_after+label_mean_after\n",
    "answer -= redness_std_after+elongation_std_after+label_std_after\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Write a function to standardize data\n",
    "It is often useful to `standardize` the columns of the data matrix to put all values in a similar scale, so to facilitate re-using this operation, write a function that takes as input a data frame `df` containing a dataset and returns a dataframe `df_scaled` with the columns standardized <b>EXCEPT THE TARGET VALUES</b> of the last column (use your answers to the previous section). Use this template for your code:\n",
    "\n",
    "    def standardize_df(df):\n",
    "    '''Standardize all the columns except the last one (target values).'''\n",
    "    # YOUR CODE HERE\n",
    "    return df_scaled\n",
    "    \n",
    "Test your function with the same dataframe as in the previous questions and use the methods `head` and `describe` to verify that all the columns are standardized, except the last one. Notice that, due to machine precision, you may not get exactly mean=0 and std=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace by your own code:\n",
    "def standardize_df(df):\n",
    "    '''Standardize all the columns except the last one (target values).'''\n",
    "    df = df_scaled # REPLACE THIS\n",
    "    return df_scaled\n",
    "\n",
    "df_scaled2 = standardize_df(df)\n",
    "df_scaled2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the checker code, keep it\n",
    "question = 2\n",
    "answer = (df_scaled == df_scaled2).all().all()\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__01(data)` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Select the best representation\n",
    "We are now going to loop over all datasets and compare the performance of our baseline method (one nearest neighbor classifier) with and without variable scaling. \n",
    "\n",
    "First you will perform these steps (with some help):\n",
    "* Import `KNeighborsClassifier` from the scikit-learn library (`sklearn`) and instanciate a one nearest neighbor classifier that you will call `sklearn_model`. Also import the `balanced_accuracy_score` and name it `sklearn_metric`.\n",
    "* Call `df_cross_validate(df, sklearn_model, sklearn_metric)` and collect the results, then print the training and test performance and their error bars. <b>Tip:</b> look at the bottom of the code using `??df_cross_validate` to get an idea on how to print the results. \n",
    "* Create:\n",
    " * a list containing all the dataset dataframes and call it `all_data_df`\n",
    " * a list containg all the dataset names and call it `data_name`. \n",
    "* Run the function `systematic_data_experiment(data_name, all_scaled_data_df, sklearn_model, sklearn_metric)` and display the results.\n",
    "* <b>Create a list</b> containing all the SCALED datasets (variables standardized) and call it `all_scaled_data_df`.\n",
    "* Run again `systematic_data_experiment` on `all_scaled_data_df` and display the results.\n",
    "* Fuse the results of the two previous question by creating a dataframe called `joint_results`. Assign `result_scaling.perf_te` to a column called `'SCALED'` and `result_noscaling.perf_te` to a column called `'NOT SCALED'`. Display the results.\n",
    "* Visualize `joint_results` with a histogram. Check <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html\">this documentation</a>.\n",
    "\n",
    "Then the questions you should answer are:\n",
    "* Does re-scaling variables always help?\n",
    "* Is which case does it help most?\n",
    "\n",
    "Try to reason why this might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Make necessary changes; USE THE DOCUMENTATION OF SCIKIT LEARN (do not copy on your neighbor).</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB # Replace that by nearest neighbors\n",
    "from sklearn.metrics import accuracy_score as sklearn_metric # Replace that by balanced accuracy \n",
    "sklearn_model = GaussianNB() # Replace by ONE nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__02()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__03()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Collect the results of df_cross_validate(df, sklearn_model, sklearn_metric).</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??df_cross_validate\n",
    "p_te, s_te, p_tr, s_tr  = 0,  0, 0, 0 # REPLACE THIS!\n",
    "\n",
    "metric_name = sklearn_metric.__name__.upper()\n",
    "print(\"AVERAGE TRAINING {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_tr, s_tr))\n",
    "print(\"AVERAGE TEST {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_te, s_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you this code, read it and try to understand it\n",
    "data_name = [os.path.basename(file)[:-9] for file in data_list]\n",
    "all_data_df = [pd.read_csv(file) for file in data_list]\n",
    "print('NO SCALING')\n",
    "result_noscaling = systematic_data_experiment(data_name, all_data_df, sklearn_model, sklearn_metric)\n",
    "result_noscaling.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe by comparing the accuracy on training data and on test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Compute all_scaled_data_df in a one-line formula. Imitate the code in the previous cell.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scaled_data_df = all_data_df # CHANGE THIS!\n",
    "\n",
    "print('WITH SCALING')\n",
    "result_scaling = systematic_data_experiment(data_name, all_scaled_data_df, sklearn_model, sklearn_metric)\n",
    "result_scaling.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to collect the test results in a single dataframe\n",
    "joint_results = pd.DataFrame()\n",
    "joint_results['SCALED'] = result_scaling.perf_te\n",
    "joint_results['NOT SCALED'] = result_noscaling.perf_te\n",
    "joint_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Put here the code to plot \"joint_results\" as a bar graph. Check the <a href=\"Check https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html\"> documentation</a></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_results.boxplot() # REPLACE THIS!\n",
    "plt.ylabel(sklearn_metric.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Your final answers to question 3:</a></b> Does rescaling always help? In which case does it help most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer to question 3 here\n",
    "rescaling_always_help = 100 # 0 for false, 1 for true\n",
    "varnum_rescaling_helps_most = -1 # num of var (0-based) for which perf. improve most after rescaling\n",
    "case_num_rescaling_helps_most = -1 # num of case (0-based) for which perf. improve most after rescaling\n",
    "\n",
    "# This is the checker code, keep it\n",
    "question = 3\n",
    "answer = (p_tr+s_tr+p_te+s_te)/10\n",
    "answer += joint_results.sum().sum()/10-rescaling_always_help-case_num_rescaling_helps_most\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__04()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__05()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify the best model\n",
    "We are now ready to perform systematic experiments on various models. We will use the methods proposed in the variable `classifiers` of <a href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\">these scikit-learn examples</a>, compare classifier performances on the dataset `CS_SCALED`:\n",
    "\n",
    "### Question 4: Overfitting and underfitting\n",
    "\n",
    "</b> There may be two reasons why a model performs poorly. It could either be <b>overfitting</b> or <b>underfitting</b> data. \n",
    "\n",
    "First you will perform these steps (with some help):\n",
    "\n",
    "* Create a variable `data_df` and assign to it the data frame of the scaled version of the CS dataset.\n",
    "* Create a variable `model_name` and a variable `model_list` containing the list of model names and the list of models (classifiers) from the <a href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\">the scikit-learn examples</a> we pointed you to.\n",
    "* Call `systematic_model_experiment(data_df, model_name, model_list, sklearn_metric)` and display the results. \n",
    "* Find which method performs best. <b>Tip:</b> Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmax.html#pandas.Series.idxmax\">idxmax</a>.\n",
    "* Find which models have TEST performance UNDER THE MEDIAN test performance. The single out models with TRAINING performance UNDER THE MEDIAN and those with TRAINING performance OVER THE MEDIAN.\n",
    "\n",
    "Then answer those questions:\n",
    "* If the test performance is bad but the training performance is good, is the model under-fitting or over-fitting? \n",
    "* If both are bad, is the model is under-fitting or over-fitting? \n",
    "* Which models are over-fitted and which ones are under-fitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__06()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__07()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Choose the correct dataset: CS representation, scaled data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_choice = 0 # Change that\n",
    "data_df = all_data_df[dataset_choice] # Change that\n",
    "\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier comparison\n",
    "We import a bunch of classifiers, inspired \n",
    "by <a href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\">that list</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "model_name = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "model_list = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=10),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Systematic experiments:\n",
    "We give you this code to make systematic experiments (run all models on the chosen representation). You may get a warning. What could explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compar_results = systematic_model_experiment(data_df, model_name, model_list, sklearn_metric)\n",
    "compar_results.round(2).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Find the method performing best on test data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the method performing best on test data\n",
    "best_method = \"AdaBoost\" # REPLACE THIS; try not to just do this by hand, apply idxmax to the series perf_te\n",
    "print(\"Best method: {}\".format(best_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-fitting and under-fitting\n",
    "There may be two reasons why a model performs poorly. It could either be <b>over-fitting</b> or <b>under-fitting</b> data. Under-fitting means that the model is not powerful enough to even learn the training data while over-fitting means that the model is so powerful that it can learn super-well the training data, but it might not generalize well to new test data.\n",
    "\n",
    "Of the models performing poorly (having TEST performance UNDER THE MEDIAN test performance), we highlight:\n",
    "* models with TRAINING performance UNDER THE MEDIAN training performance (<b>under-fitted</b>)\n",
    "* models with TRAINING performance OVER THE MEDIAN (<b>over-fitted</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you this code, check it in utilities.py of use ??analyze_model_experiments\n",
    "analyze_model_experiments(compar_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar graph comparing  results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compar_results[['perf_tr', 'perf_te']].plot.bar()\n",
    "plt.ylim(0.5, 1)\n",
    "plt.ylabel(sklearn_metric.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>\n",
    "<b>Your final answers to question 4:</a> </b>Which methods overfits or underfit?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer to question 4 here\n",
    "#If the test performance is bad but the training performance is good, is the model under-fitting or over-fitting?\n",
    "answer1 = -1 # 0 for under-fitting and 1 for over-fitting\n",
    "#If both are bad, is the model is under-fitting or over-fitting? \n",
    "answer2 = -1 # 0 for under-fitting and 1 for over-fitting\n",
    "# Which models are over-fitted and which ones are under-fitted?\n",
    "overfitted_list = [1,3,6] # Replace by the correct numbers in model_name\n",
    "underfitted_list = [2,8] # Replace by the correct numbers in model_name\n",
    "\n",
    "model_name = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "print(\"Over_fitted models:\")\n",
    "print([model_name[i] for i in overfitted_list])\n",
    "print(\"Under_fitted models:\")\n",
    "print([model_name[i] for i in underfitted_list])\n",
    "# This is the checker code, keep it\n",
    "question = 4\n",
    "answer = answer1-answer2+sum(overfitted_list)-sum(underfitted_list)+model_name.index(best_method)\n",
    "\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__08()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "    <b>You can now fill `answer__09()` in answer.py</b>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 5: Dimensionality reduction\n",
    "It is useful to reduce the data dimension (number of features) for two reasons: ease of visualisation and possibly increase in performance. Two approaches are possible:\n",
    "1. Feature selection.\n",
    "1. Feature transforms.\n",
    "\n",
    "In the first case, one tries to select among the original features. In the second case, one first replaces the original features by \"combinations\" of features, then perform selection. Here we give two simple examples: Feature ranking with the <b>Pearson correlation</b> coefficient (as a feature selection method), and <b>Singular Value Decomposition</b> or SVD (as a feature transform method) .\n",
    "\n",
    "In this section, we keep using the <b>CS scaled data</b>. We use as classifier the <b>3-nearest-neighbor</b> model, which seems to have served us well! We begin by showing you how to do simple feature selection, then we'll guide you step-by-step through SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection example\n",
    "We perform feature selection with the Pearson correlation coefficient as follows:\n",
    "* Compute the correlation matrix (we will only use the last column, i.e. correlation of features with the class label to be predicted)\n",
    "* Sort the correlation coefficients (of features and class label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix\n",
    "If we did things right, `data_df` should contain the CS data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_df.corr()\n",
    "corr.round(1).style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first sort all features by the <b>absolute value</b> of the Pearson correlation coefficient. Indeed, variables are informative no matter whether they are correlated or anti-correlated (since it suffices to multiply them by -1 to change the correlation direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sval = corr['fruit'][:-1].abs().sort_values(ascending=False)\n",
    "ranked_columns = sval.index.values\n",
    "print(ranked_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the features that we have constructed in the previous lessons 'R-(G+B)/2' and 'W/H' come in the 5 top most informative features. But there are others. Let us make all scatter plots of pairs of features for the 5 top ranked features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize top 5 features with PAIRPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe with only top five features\n",
    "fruit_name = ['Banana', 'Apple']\n",
    "fruit_list = [fruit_name[int((i+1)/2)] for i in data_df[\"fruit\"].tolist()]\n",
    "col_selected = ranked_columns[0:5]\n",
    "df_5feat = pd.DataFrame.copy(data_df)\n",
    "df_5feat = df_5feat[col_selected]\n",
    "df_5feat['fruit'] = fruit_list\n",
    "df_5feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pairplot\n",
    "g = sns.pairplot(df_5feat, hue=\"fruit\", markers=[\"o\", \"s\"], diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves\n",
    "We want to eveluate the effect of varying the number of features. To that end, we build learning curves = performance as a function of feature number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the 3 nearest neighbor classifier to create the learnign curve\n",
    "sklearn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "feat_lc_df = feature_learning_curve(data_df, sklearn_model, sklearn_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.errorbar(feat_lc_df.index+1, feat_lc_df['perf_tr'], yerr=feat_lc_df['std_tr'], label='Training set')\n",
    "plt.errorbar(feat_lc_df.index+1, feat_lc_df['perf_te'], yerr=feat_lc_df['std_te'], label='Test set')\n",
    "plt.xticks(np.arange(1, 22, 1)) \n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel(sklearn_metric.__name__)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the, with 5 features, it is about as good as it gets, given the error bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "Let us move now to the second way of reducing dimensionality: feature transforms.\n",
    "When we use SVD, the feature transform consists in finding the \"principal directions\"(directions of largest variance). The method is also known as Pricipal Component Analysis (PCA). I simply used my search engine and typed the keywords \"pandas svd\". I found a nice tutorial on <a href=\"https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/\">this page</a> and a step-by-step procedure on <a href=\"https://cmdlinetips.com/2019/05/singular-value-decomposition-svd-in-python/\">this page</a>. \n",
    "\n",
    "We will guide you step-by-step:\n",
    "* Create a dataframe called `df_scaled` containing the standardized columns, except the last one (tip: just use `drop` to eliminate the last column).\n",
    "* Perform a singular value decomposition of `df_scaled` and call the resulting matrices u, s, v.\n",
    "* Make a scree plot of the eigen values (square of the singular values). Save the plot in file 'svd_scree_plot.png'.\n",
    "* Create a new dataframe `svd_df` with the two singular values as columns and the fruit type as index.\n",
    "* Make pairwise scatter plots of the three first singular values.\n",
    "\n",
    "Then the question to answer will be to compute the performances obtained with the 3 nearest neighbor method using the first 3 singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use the CS scaled dataset (this should be already loaded in data_df,\n",
    "# if you did things correctly in previous questions)\n",
    "# To obtain df_scaled, remove the last column of data_df.\n",
    "\n",
    "df_scaled = df_scaled2[{\"redness\", \"elongation\"}] # REPLACE THIS BY THE CORRECT ANSWER\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(df_scaled, full_matrices=True)\n",
    "print('U {}'.format(u.shape))\n",
    "print('S {}'.format(s.shape))\n",
    "print('V {}'.format(v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a scree plot\n",
    "This plot allows us to decide how many components to keep, considering the total variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_explained = np.round(s**2/np.sum(s**2), decimals=3)\n",
    "sns.barplot(x=list(range(1,len(var_explained)+1)),\n",
    "            y=var_explained, color=\"limegreen\")\n",
    "plt.xlabel('SVs', fontsize=16)\n",
    "plt.ylabel('Percent Variance Explained', fontsize=16)\n",
    "plt.savefig('svd_scree_plot.png',dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after 3 features there is a big drop in variance. So tentatively, let us keep the top three components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with only the three first principal components\n",
    "fruit_name = ['Banana', 'Apple']\n",
    "fruit_list = [fruit_name[int((i+1)/2)] for i in df[\"fruit\"].tolist()]\n",
    "fnum=3\n",
    "labels= ['SV'+str(i) for i in range(1,fnum+1)]\n",
    "df_3svd = pd.DataFrame(u[:,0:fnum], columns=labels)\n",
    "df_3svd['fruit'] = fruit_list\n",
    "df_3svd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pairplot of svd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate df_3svd: \n",
    "This piece of code will serve you later to answer question 5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the function df_cross_validate that we already used in question 3 to evaluate svd_df\n",
    "df_3svd['fruit'] = df.iloc[:, -1].to_numpy() # We replace the target values by numbers\n",
    "p_tr, s_tr, p_te, s_te = df_cross_validate(df_3svd, sklearn_model, sklearn_metric)\n",
    "metric_name = sklearn_metric.__name__.upper()\n",
    "print(\"AVERAGE TRAINING {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_tr, s_tr))\n",
    "print(\"AVERAGE TEST {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_te, s_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the learning curve for the 3-nearest neighbor classifier ...\n",
    "sklearn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "feat_lc_df = svd_learning_curve(data_df, sklearn_model, sklearn_metric)\n",
    "\n",
    "# and we plot the learning curve\n",
    "plt.errorbar(feat_lc_df.index+1, feat_lc_df['perf_tr'], yerr=feat_lc_df['std_tr'], label='Training set')\n",
    "plt.errorbar(feat_lc_df.index+1, feat_lc_df['perf_te'], yerr=feat_lc_df['std_te'], label='Test set')\n",
    "plt.xticks(np.arange(1, 22, 1)) \n",
    "plt.xlabel('Number of principal compoments')\n",
    "plt.ylabel(sklearn_metric.__name__)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of dimensionality reduction methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though features 4 and 5 did not seem to explain a lot of additional variance, there seems to be an optimum at 4 or 5 features. Compare the performances of df_5feat and df_5svd (with 5 top components). Is one significantly better then the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Here you need to do something!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5svd = df_3svd # Replace with the correct dataframe having 5 principal components\n",
    "\n",
    "# Put here the code to compute the performances of df_5feat and df_5svd\n",
    "fnum=5\n",
    "labels= ['SV'+str(i) for i in range(1,fnum+1)]\n",
    "df_5svd = pd.DataFrame(u[:,0:fnum], columns=labels)\n",
    "df_5svd['fruit'] = df.iloc[:, -1].to_numpy() # Need numeric values\n",
    "p_tr_5svd, s_tr_5svd, p_te_5svd, s_te_5svd = 0,0,0,0 # REPLACE THIS\n",
    "p_tr_5feat, s_tr_5feat, p_te_5feat, s_te_5feat = 0,0,0,0 # REPLACE THIS\n",
    "\n",
    "print(\"5FEAT AVERAGE TRAINING {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_tr_5feat, s_tr_5feat))\n",
    "print(\"5FEAT AVERAGE TEST {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_te_5feat, s_te_5feat))\n",
    "print(\"5SVD AVERAGE TRAINING {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_tr_5svd, s_tr_5svd))\n",
    "print(\"5SVD AVERAGE TEST {0:s} +- STD: {1:.2f} +- {2:.2f}\".format(metric_name, p_te_5svd, s_te_5svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does one method give significantly better results than the other?\n",
    "answer = -1 # One for yes and 0 for no\n",
    "\n",
    "# This is the checker code, keep it\n",
    "question = 5\n",
    "answer = 0.001*(p_te_5feat-p_te_5svd)/(s_te_5feat+s_te_5svd)\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Your final score is %d / 5, congratulations!' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<span style=\"color:red\">\n",
    "<br>\n",
    "    To finalize your homework:\n",
    "<b>\n",
    "<ul>\n",
    "    <li> Use  Kernel + Restart and Run all.</li>\n",
    "    <li> Save your notebook.</li>\n",
    "    <li> Push your changes to your GitHub repo with:</li>\n",
    "</ul>   \n",
    "</b>\n",
    "<pre>\n",
    "git add .\n",
    "git commit -m 'my homework is done'\n",
    "git push\n",
    "</pre>\n",
    "<br>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
